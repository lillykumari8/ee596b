{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract MNIST data</h2>\n",
    "<p style=\"font-size:20px\">You can change the option of one_hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = mnist.train.num_examples #55,000\n",
    "num_validation = mnist.validation.num_examples #5000\n",
    "num_test = mnist.test.num_examples #10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps =10000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 1200\n",
    "n_hidden_2 = 1200\n",
    "# n_hidden_3 = 100\n",
    "num_input = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define placeholder and Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# #tf graph input\n",
    "# X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "# Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "# #Layers weight & bias\n",
    "# weights = {\n",
    "#     'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "#     'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "#     'W3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3]),name='W3'),\n",
    "#     'Wout': tf.Variable(tf.random_normal([n_hidden_3, num_classes]),name='Wout')\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "#     'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "#     'b3': tf.Variable(tf.zeros(shape=[n_hidden_3]),name='b3'),\n",
    "#     'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define neural network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define a neural net model\n",
    "# def neural_net(x):\n",
    "#     layer_1_out = tf.nn.sigmoid(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "#     layer_2_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "#     layer_3_out = tf.nn.relu(tf.add(tf.matmul(layer_2_out,weights['W3']),biases['b3']))\n",
    "#     out = tf.add(tf.matmul(layer_3_out,weights['Wout']),biases['bout'])\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define cost function and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(tf.nn.softmax(logits),1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, Accuracy= 0.367\n",
      "step 500, Accuracy= 0.992\n",
      "step 1000, Accuracy= 0.992\n",
      "step 1500, Accuracy= 1.000\n",
      "step 2000, Accuracy= 1.000\n",
      "step 2500, Accuracy= 1.000\n",
      "step 3000, Accuracy= 1.000\n",
      "step 3500, Accuracy= 1.000\n",
      "step 4000, Accuracy= 1.000\n",
      "step 4500, Accuracy= 1.000\n",
      "step 5000, Accuracy= 1.000\n",
      "step 5500, Accuracy= 1.000\n",
      "step 6000, Accuracy= 1.000\n",
      "step 6500, Accuracy= 1.000\n",
      "step 7000, Accuracy= 1.000\n",
      "step 7500, Accuracy= 1.000\n",
      "step 8000, Accuracy= 1.000\n",
      "step 8500, Accuracy= 1.000\n",
      "step 9000, Accuracy= 1.000\n",
      "step 9500, Accuracy= 1.000\n",
      "Training finished!\n",
      "Testing Accuracy: 0.9536\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        if i % 500 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Your results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps =12000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 1200\n",
    "n_hidden_2 = 1200 \n",
    "\n",
    "num_input = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.float32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([num_input, n_hidden_1], stddev=0.1),name='W1'),\n",
    "    'W2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1),name='W2'),\n",
    "    'Wout': tf.Variable(tf.truncated_normal([n_hidden_2, num_classes], stddev=0.1),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "\n",
    "#define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(tf.nn.softmax(logits),1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, Accuracy= 0.156\n",
      "Testing ACcuracy: 0.1396\n",
      "step 500, Accuracy= 0.906\n",
      "step 1000, Accuracy= 0.969\n",
      "Testing ACcuracy: 0.9359\n",
      "step 1500, Accuracy= 0.977\n",
      "step 2000, Accuracy= 0.977\n",
      "Testing ACcuracy: 0.948\n",
      "step 2500, Accuracy= 0.945\n",
      "step 3000, Accuracy= 0.969\n",
      "Testing ACcuracy: 0.9569\n",
      "step 3500, Accuracy= 0.984\n",
      "step 4000, Accuracy= 0.992\n",
      "Testing ACcuracy: 0.9591\n",
      "step 4500, Accuracy= 0.969\n",
      "step 5000, Accuracy= 0.992\n",
      "Testing ACcuracy: 0.962\n",
      "step 5500, Accuracy= 1.000\n",
      "step 6000, Accuracy= 1.000\n",
      "Testing ACcuracy: 0.9655\n",
      "step 6500, Accuracy= 1.000\n",
      "step 7000, Accuracy= 1.000\n",
      "Testing ACcuracy: 0.9661\n",
      "step 7500, Accuracy= 0.984\n",
      "step 8000, Accuracy= 1.000\n",
      "Testing ACcuracy: 0.9659\n",
      "step 8500, Accuracy= 1.000\n",
      "step 9000, Accuracy= 1.000\n",
      "Testing ACcuracy: 0.9692\n",
      "step 9500, Accuracy= 1.000\n",
      "step 10000, Accuracy= 1.000\n",
      "Testing ACcuracy: 0.9694\n",
      "step 10500, Accuracy= 0.992\n",
      "step 11000, Accuracy= 1.000\n",
      "Testing ACcuracy: 0.9694\n",
      "step 11500, Accuracy= 1.000\n",
      "Training finished!\n",
      "Testing ACcuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        if i % 500 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "        if i%1000 == 0:\n",
    "            print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "| .  Settings            |Acc . |\n",
    "|------------------------|------|\n",
    "| lr = 0.01              |0.8034|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 300, relu  |      |\n",
    "|n_hidden_2 = 100, relu  |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.1               |0.2392|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 300, relu  |      |\n",
    "|n_hidden_2 = 100, relu  |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.001             |0.865 |\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 300, relu  |      |\n",
    "|n_hidden_2 = 100, relu  |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.001             |0.8754|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 256        |      |\n",
    "|n_hidden_1 = 300, relu  |      |\n",
    "|n_hidden_2 = 100, relu  |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.001             |0.2339|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 32, relu   |      |\n",
    "|n_hidden_2 = 16, relu   |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.001             |0.9376|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 1000, relu |      |\n",
    "|n_hidden_2 = 600, relu  |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.01              |0.9362|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 1000, relu |      |\n",
    "|n_hidden_2 = 600, relu  |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.01              |0.9561|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 1000, relu |      |\n",
    "|n_hidden_2 = 1000, relu |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.01              |0.8127|\n",
    "|num_steps =10000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1=1000,sigmoid |      |\n",
    "|n_hidden_2=500,sigmoid  |      |\n",
    "|n_hidden_3=100, relu    |      |\n",
    "|------------------------|------|\n",
    "| lr = 0.01              |0.9703|\n",
    "|num_steps =12000        |      |\n",
    "|batch_size = 128        |      |\n",
    "|n_hidden_1 = 1200, relu |      |\n",
    "|n_hidden_2 = 1200, relu |      |\n",
    "|different weight init   |      |\n",
    "|------------------------|------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Table for settings vs accuracy added in HW1_template directory as RAW NBconvert not working in git \n",
    "\n",
    "Parameters contributing most to the improvement:\n",
    "* Increasing number of neurons in 1st and 2nd hidden layer\n",
    "* Learning rate of 0.01\n",
    "* Changing weights initialization setting from random_normal to truncated_normal\n",
    "* Using relu activation instead of sigmoid\n",
    "\n",
    "\n",
    "Parameters not contributing much to the improvement:\n",
    "* Batch size changes\n",
    "* increasing the number of hidden layers beyond 2\n",
    "\n",
    "\n",
    "Changing the learning rate (i.e. increasing to values like 0.1) has negative impact on the test accuracy as the model has divergent behavior then. \n",
    "\n",
    "Batch size updates (between 256 and 128) does not affect the model's final performance.\n",
    "\n",
    "Using RELU activation function helps as it leads to a better convergence and also helps in eliminating the vanishing gradient problem associated with sigmoid.\n",
    "\n",
    "Using same number of neurons in the 1st and 2nd hidden layers helps improve the final performance as it increases the overall number of model parameters which are to be learnt from the training data.\n",
    "\n",
    "Increasing the number of neurons in the hidden layer 1 and 2 improves the final performance too because it leads to an increase in the trainable model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
