{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit, time, math\n",
    "from sklearn.utils import shuffle\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extract MNIST data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding, reshape = False (that means images are not flatten)\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",reshape=False,one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare training, validation and testing data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "x_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "x_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "#pad images with 0s (28x28 to 32x32)\n",
    "x_train = np.pad(x_train, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
    "x_validation = np.pad(x_validation, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
    "\n",
    "print (x_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001\n",
    "\n",
    "batches = math.floor(x_train.shape[0] // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 32, 32, 1])\n",
    "Y = tf.placeholder(tf.int32, [None, 10])\n",
    "\n",
    "\n",
    "mean = 0.0\n",
    "stddev = 0.1\n",
    "\n",
    "def initialize_weight(shape, mean, stddev):\n",
    "    W = tf.truncated_normal(shape=shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(W)\n",
    "\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'W_conv1': tf.Variable(initialize_weight([5,5,1,6], mean, stddev),name='W_conv1'),\n",
    "    'W_conv2': tf.Variable(initialize_weight([5,5,6,16], mean, stddev),name='W_conv2'),\n",
    "    'W_fc1': tf.Variable(initialize_weight([5*5*16,120], mean, stddev),name='W_fc1'),\n",
    "    'W_fc2': tf.Variable(initialize_weight([120,84], mean, stddev),name='W_fc2'),\n",
    "    'W_out': tf.Variable(initialize_weight([84,num_classes], mean, stddev),name='W_out')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b_conv1': tf.Variable(tf.zeros(shape=[6]),name='b_conv1'),\n",
    "    'b_conv2': tf.Variable(tf.zeros(shape=[16]),name='b_conv2'),\n",
    "    'b_fc1': tf.Variable(tf.zeros(shape=[120]),name='b_fc1'),\n",
    "    'b_fc2': tf.Variable(tf.zeros(shape=[84]),name='b_fc2'),\n",
    "    'b_out': tf.Variable(tf.zeros(shape=[num_classes]),name='b_out')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define LeNet-5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet(x):\n",
    "    conv1 = tf.nn.conv2d(x, weights['W_conv1'], strides=[1,1,1,1], padding='VALID') + biases['b_conv1']\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(conv1, weights['W_conv2'], strides=[1,1,1,1], padding='VALID') + biases['b_conv2']\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    fc_input = tf.contrib.layers.flatten(conv2)\n",
    "    \n",
    "    fc1 = tf.matmul(fc_input, weights['W_fc1']) + biases['b_fc1']\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc2 = tf.matmul(fc1, weights['W_fc2']) + biases['b_fc2']\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    out = tf.matmul(fc2, weights['W_out']) + biases['b_out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = LeNet(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training, validating, testing</h1>\n",
    "<h2>1. Print out validation accuracy after each training epoch</h2>\n",
    "<h2>2. Print out training time on each epoch</h2>\n",
    "<h2>3. Print out testing accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss = 0.13400030136108398, training_time = 2.7117762565612793, val_accuracy = 0.9664000272750854, test_accuracy = 0.9624999761581421\n",
      "Epoch 1: training loss = 0.0984925702214241, training_time = 2.513347864151001, val_accuracy = 0.9753999710083008, test_accuracy = 0.9779999852180481\n",
      "Epoch 2: training loss = 0.07610903680324554, training_time = 2.5133113861083984, val_accuracy = 0.9837999939918518, test_accuracy = 0.9830999970436096\n",
      "Epoch 3: training loss = 0.029794372618198395, training_time = 2.4452459812164307, val_accuracy = 0.9860000014305115, test_accuracy = 0.9876999855041504\n",
      "Epoch 4: training loss = 0.04230296611785889, training_time = 2.4303364753723145, val_accuracy = 0.9829999804496765, test_accuracy = 0.9861000180244446\n",
      "Epoch 5: training loss = 0.01900586672127247, training_time = 2.431164264678955, val_accuracy = 0.9879999756813049, test_accuracy = 0.989799976348877\n",
      "Epoch 6: training loss = 0.035759564489126205, training_time = 2.5024423599243164, val_accuracy = 0.9876000285148621, test_accuracy = 0.9886000156402588\n",
      "Epoch 7: training loss = 0.06433894485235214, training_time = 2.5651004314422607, val_accuracy = 0.9872000217437744, test_accuracy = 0.9897000193595886\n",
      "Epoch 8: training loss = 0.003906572703272104, training_time = 2.5770788192749023, val_accuracy = 0.9890000224113464, test_accuracy = 0.9876000285148621\n",
      "Epoch 9: training loss = 0.01776275970041752, training_time = 2.5307698249816895, val_accuracy = 0.9869999885559082, test_accuracy = 0.9886999726295471\n",
      "Epoch 10: training loss = 0.028691377490758896, training_time = 2.3537228107452393, val_accuracy = 0.9869999885559082, test_accuracy = 0.9902999997138977\n",
      "Epoch 11: training loss = 0.027810372412204742, training_time = 2.3539469242095947, val_accuracy = 0.9879999756813049, test_accuracy = 0.9901999831199646\n",
      "Epoch 12: training loss = 0.07351185381412506, training_time = 2.3554439544677734, val_accuracy = 0.9878000020980835, test_accuracy = 0.9901000261306763\n",
      "Epoch 13: training loss = 0.005991310812532902, training_time = 2.3310365676879883, val_accuracy = 0.98580002784729, test_accuracy = 0.9886999726295471\n",
      "Epoch 14: training loss = 0.0011730861151590943, training_time = 2.358792781829834, val_accuracy = 0.989799976348877, test_accuracy = 0.9894999861717224\n",
      "Epoch 15: training loss = 0.0032214701641350985, training_time = 2.3726508617401123, val_accuracy = 0.9887999892234802, test_accuracy = 0.9896000027656555\n",
      "Epoch 16: training loss = 0.0022099886555224657, training_time = 2.353102922439575, val_accuracy = 0.9890000224113464, test_accuracy = 0.9898999929428101\n",
      "Epoch 17: training loss = 0.0019798572175204754, training_time = 2.353072166442871, val_accuracy = 0.9896000027656555, test_accuracy = 0.9900000095367432\n",
      "Epoch 18: training loss = 0.013454439118504524, training_time = 2.357983112335205, val_accuracy = 0.9864000082015991, test_accuracy = 0.9861000180244446\n",
      "Epoch 19: training loss = 0.008992312476038933, training_time = 2.3690128326416016, val_accuracy = 0.9894000291824341, test_accuracy = 0.9902999997138977\n",
      "Epoch 20: training loss = 0.0014514941722154617, training_time = 2.3565587997436523, val_accuracy = 0.9918000102043152, test_accuracy = 0.9922000169754028\n",
      "Epoch 21: training loss = 0.0026339346077293158, training_time = 2.3377678394317627, val_accuracy = 0.9900000095367432, test_accuracy = 0.9887999892234802\n",
      "Epoch 22: training loss = 7.082868251018226e-05, training_time = 2.342155694961548, val_accuracy = 0.9886000156402588, test_accuracy = 0.9890000224113464\n",
      "Epoch 23: training loss = 0.0028385675977915525, training_time = 2.3511791229248047, val_accuracy = 0.9890000224113464, test_accuracy = 0.9891999959945679\n",
      "Epoch 24: training loss = 0.007504299748688936, training_time = 2.3488755226135254, val_accuracy = 0.9882000088691711, test_accuracy = 0.9890999794006348\n",
      "Epoch 25: training loss = 0.0001914952154038474, training_time = 2.3381638526916504, val_accuracy = 0.9891999959945679, test_accuracy = 0.9887999892234802\n",
      "Epoch 26: training loss = 0.013041282072663307, training_time = 2.3354501724243164, val_accuracy = 0.9882000088691711, test_accuracy = 0.9873999953269958\n",
      "Epoch 27: training loss = 0.00350710260681808, training_time = 2.331468105316162, val_accuracy = 0.9886000156402588, test_accuracy = 0.9890000224113464\n",
      "Epoch 28: training loss = 0.0008274980937130749, training_time = 2.2966885566711426, val_accuracy = 0.989799976348877, test_accuracy = 0.9890999794006348\n",
      "Epoch 29: training loss = 0.0001683036971371621, training_time = 2.3089518547058105, val_accuracy = 0.989799976348877, test_accuracy = 0.989300012588501\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = []\n",
    "test_accuracy = []\n",
    "time_taken = []\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for ep in range(EPOCHS):\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation, :]\n",
    "        y_train_shuffled = y_train[permutation, :]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for j in range(batches):\n",
    "\n",
    "            start = j * BATCH_SIZE\n",
    "            end = min(start + BATCH_SIZE, x_train.shape[0] - 1)\n",
    "            X_batch = x_train_shuffled[start:end]\n",
    "            Y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            train_loss, acc, _ = sess.run(fetches=[loss, accuracy, train_op],\n",
    "                                         feed_dict={X: X_batch,\n",
    "                                                   Y: Y_batch})\n",
    "        end_time = time.time()\n",
    "        time_s = end_time-start_time\n",
    " \n",
    "        val_acc = sess.run([accuracy], feed_dict={X:x_validation,\n",
    "                                                 Y: y_validation})\n",
    "    \n",
    "        test_acc = sess.run([accuracy], feed_dict={X: x_test,\n",
    "                                                  Y: y_test})\n",
    "        print(\"Epoch {}: training loss = {}, training_time = {}, val_accuracy = {}, test_accuracy = {}\".format(\n",
    "        ep, train_loss, time_s, val_acc[0], test_acc[0]))\n",
    "        \n",
    "        \n",
    "        val_accuracy.append(val_acc[0])\n",
    "        test_accuracy.append(test_acc[0])\n",
    "        time_taken.append(time_s)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
