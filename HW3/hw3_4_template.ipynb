{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from text_utils import TextLoader\n",
    "from tensorflow.contrib import rnn\n",
    "from char_rnn_model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directories, hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "batch_size = 256\n",
    "\n",
    "seq_len = 50\n",
    "rnn_size = 150\n",
    "rnn_type = 'rnn'\n",
    "num_layers = 4\n",
    "use_dropout = False\n",
    "lr = 0.002\n",
    "is_training = True\n",
    "input_keep_prob = 1.0\n",
    "outptut_keep_prob = 1.0\n",
    "use_embedding = True\n",
    "embedding_size = 50\n",
    "max_grad_clip = 5.0\n",
    "\n",
    "data_directory = '/home/jupyter/HW3_template/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data using TextLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader = TextLoader(data_directory, batch_size, seq_len)\n",
    "vocab_size = text_loader.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/HW3_template/char_rnn_model.py:65: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/jupyter/HW3_template/char_rnn_model.py:74: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "HELLOI\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jupyter/HW3_template/char_rnn_model.py:92: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "is_training = True\n",
    "model = Model(batch_size, seq_len, vocab_size, rnn_size, rnn_type, num_layers, use_dropout, lr, is_training, input_keep_prob, outptut_keep_prob, use_embedding, embedding_size, max_grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = data_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initial_state\n",
    "batch_data = text_loader.generate_batches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7f12a029b0b8>\n",
      "[<tf.Variable 'embedding:0' shape=(67, 50) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(200, 150) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_1/basic_rnn_cell/kernel:0' shape=(300, 150) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_1/basic_rnn_cell/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_2/basic_rnn_cell/kernel:0' shape=(300, 150) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_2/basic_rnn_cell/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_3/basic_rnn_cell/kernel:0' shape=(300, 150) dtype=float32_ref>, <tf.Variable 'model/multi_rnn_cell/cell_3/basic_rnn_cell/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'weights_last_layer:0' shape=(150, 67) dtype=float32_ref>, <tf.Variable 'bias_last_layer:0' shape=(67,) dtype=float32_ref>]\n",
      "Epoch: 0, Batch_num: 0, Time = 0.554694414, Training_loss = 4.24293\n",
      "Epoch: 0, Batch_num: 100, Time = 5.391587973, Training_loss = 2.30870\n",
      "Epoch: 0, Batch_num: 200, Time = 5.254212379, Training_loss = 2.04839\n",
      "Epoch: 0, Batch_num: 300, Time = 5.175616980, Training_loss = 1.95543\n",
      "Epoch: 1, Batch_num: 0, Time = 0.062853098, Training_loss = 1.95938\n",
      "Epoch: 1, Batch_num: 100, Time = 5.225099087, Training_loss = 1.78649\n",
      "Epoch: 1, Batch_num: 200, Time = 5.269338608, Training_loss = 1.75341\n",
      "Epoch: 1, Batch_num: 300, Time = 5.269935131, Training_loss = 1.73363\n",
      "Epoch: 2, Batch_num: 0, Time = 0.065042973, Training_loss = 1.79073\n",
      "Epoch: 2, Batch_num: 100, Time = 5.239575148, Training_loss = 1.64573\n",
      "Epoch: 2, Batch_num: 200, Time = 5.229611874, Training_loss = 1.63254\n",
      "Epoch: 2, Batch_num: 300, Time = 5.230577946, Training_loss = 1.62619\n",
      "Epoch: 3, Batch_num: 0, Time = 0.064199209, Training_loss = 1.72689\n",
      "Epoch: 3, Batch_num: 100, Time = 5.239025831, Training_loss = 1.58098\n",
      "Epoch: 3, Batch_num: 200, Time = 5.240345001, Training_loss = 1.58049\n",
      "Epoch: 3, Batch_num: 300, Time = 5.271228313, Training_loss = 1.58082\n",
      "Epoch: 4, Batch_num: 0, Time = 0.065369129, Training_loss = 1.69626\n",
      "Epoch: 4, Batch_num: 100, Time = 5.266688108, Training_loss = 1.54670\n",
      "Epoch: 4, Batch_num: 200, Time = 5.285472631, Training_loss = 1.54338\n",
      "Epoch: 4, Batch_num: 300, Time = 5.266875744, Training_loss = 1.55112\n",
      "Epoch: 5, Batch_num: 0, Time = 0.063313007, Training_loss = 1.65407\n",
      "Epoch: 5, Batch_num: 100, Time = 5.292835474, Training_loss = 1.51486\n",
      "Epoch: 5, Batch_num: 200, Time = 5.282743216, Training_loss = 1.51610\n",
      "Epoch: 5, Batch_num: 300, Time = 5.276177883, Training_loss = 1.53014\n",
      "Epoch: 6, Batch_num: 0, Time = 0.065160275, Training_loss = 1.63539\n",
      "Epoch: 6, Batch_num: 100, Time = 5.274210215, Training_loss = 1.49840\n",
      "Epoch: 6, Batch_num: 200, Time = 5.280215025, Training_loss = 1.50172\n",
      "Epoch: 6, Batch_num: 300, Time = 5.271492481, Training_loss = 1.51149\n",
      "Epoch: 7, Batch_num: 0, Time = 0.068156004, Training_loss = 1.62722\n",
      "Epoch: 7, Batch_num: 100, Time = 5.275094509, Training_loss = 1.48629\n",
      "Epoch: 7, Batch_num: 200, Time = 5.248885870, Training_loss = 1.48838\n",
      "Epoch: 7, Batch_num: 300, Time = 5.242241859, Training_loss = 1.49933\n",
      "Epoch: 8, Batch_num: 0, Time = 0.063535690, Training_loss = 1.61773\n",
      "Epoch: 8, Batch_num: 100, Time = 5.285772562, Training_loss = 1.47579\n",
      "Epoch: 8, Batch_num: 200, Time = 5.263802528, Training_loss = 1.48410\n",
      "Epoch: 8, Batch_num: 300, Time = 5.280711412, Training_loss = 1.48630\n",
      "Epoch: 9, Batch_num: 0, Time = 0.064215422, Training_loss = 1.60525\n",
      "Epoch: 9, Batch_num: 100, Time = 5.257547617, Training_loss = 1.46891\n",
      "Epoch: 9, Batch_num: 200, Time = 5.256754160, Training_loss = 1.47096\n",
      "Epoch: 9, Batch_num: 300, Time = 5.246030331, Training_loss = 1.47700\n",
      "Epoch: 10, Batch_num: 0, Time = 0.064248323, Training_loss = 1.60149\n",
      "Epoch: 10, Batch_num: 100, Time = 5.295592785, Training_loss = 1.46220\n",
      "Epoch: 10, Batch_num: 200, Time = 5.279431820, Training_loss = 1.46433\n",
      "Epoch: 10, Batch_num: 300, Time = 5.278213263, Training_loss = 1.47185\n",
      "Epoch: 11, Batch_num: 0, Time = 0.065406322, Training_loss = 1.59507\n",
      "Epoch: 11, Batch_num: 100, Time = 5.275433302, Training_loss = 1.46165\n",
      "Epoch: 11, Batch_num: 200, Time = 5.260077715, Training_loss = 1.45848\n",
      "Epoch: 11, Batch_num: 300, Time = 5.259841919, Training_loss = 1.46442\n",
      "Epoch: 12, Batch_num: 0, Time = 0.064847469, Training_loss = 1.59219\n",
      "Epoch: 12, Batch_num: 100, Time = 5.260765553, Training_loss = 1.45981\n",
      "Epoch: 12, Batch_num: 200, Time = 5.275655031, Training_loss = 1.45032\n",
      "Epoch: 12, Batch_num: 300, Time = 5.245597839, Training_loss = 1.46195\n",
      "Epoch: 13, Batch_num: 0, Time = 0.064457893, Training_loss = 1.59079\n",
      "Epoch: 13, Batch_num: 100, Time = 5.244727135, Training_loss = 1.44741\n",
      "Epoch: 13, Batch_num: 200, Time = 5.171528816, Training_loss = 1.44829\n",
      "Epoch: 13, Batch_num: 300, Time = 5.266261816, Training_loss = 1.45909\n",
      "Epoch: 14, Batch_num: 0, Time = 0.064141512, Training_loss = 1.58156\n",
      "Epoch: 14, Batch_num: 100, Time = 5.286651611, Training_loss = 1.44590\n",
      "Epoch: 14, Batch_num: 200, Time = 5.288045406, Training_loss = 1.44239\n",
      "Epoch: 14, Batch_num: 300, Time = 5.269917727, Training_loss = 1.45795\n",
      "Epoch: 15, Batch_num: 0, Time = 0.063880920, Training_loss = 1.57888\n",
      "Epoch: 15, Batch_num: 100, Time = 5.264130831, Training_loss = 1.44179\n",
      "Epoch: 15, Batch_num: 200, Time = 5.204417706, Training_loss = 1.43672\n",
      "Epoch: 15, Batch_num: 300, Time = 5.232659578, Training_loss = 1.45037\n",
      "Epoch: 16, Batch_num: 0, Time = 0.063497305, Training_loss = 1.57354\n",
      "Epoch: 16, Batch_num: 100, Time = 5.292518854, Training_loss = 1.44515\n",
      "Epoch: 16, Batch_num: 200, Time = 5.278445005, Training_loss = 1.43727\n",
      "Epoch: 16, Batch_num: 300, Time = 5.293261290, Training_loss = 1.45052\n",
      "Epoch: 17, Batch_num: 0, Time = 0.062793493, Training_loss = 1.57067\n",
      "Epoch: 17, Batch_num: 100, Time = 5.169445753, Training_loss = 1.44161\n",
      "Epoch: 17, Batch_num: 200, Time = 5.170237064, Training_loss = 1.43296\n",
      "Epoch: 17, Batch_num: 300, Time = 5.284567833, Training_loss = 1.44631\n",
      "Epoch: 18, Batch_num: 0, Time = 0.063114405, Training_loss = 1.56803\n",
      "Epoch: 18, Batch_num: 100, Time = 5.261172533, Training_loss = 1.43570\n",
      "Epoch: 18, Batch_num: 200, Time = 5.287535429, Training_loss = 1.42961\n",
      "Epoch: 18, Batch_num: 300, Time = 5.252018452, Training_loss = 1.44423\n",
      "Epoch: 19, Batch_num: 0, Time = 0.064314842, Training_loss = 1.56343\n",
      "Epoch: 19, Batch_num: 100, Time = 5.306984901, Training_loss = 1.43928\n",
      "Epoch: 19, Batch_num: 200, Time = 5.294365883, Training_loss = 1.42879\n",
      "Epoch: 19, Batch_num: 300, Time = 5.266204834, Training_loss = 1.43949\n",
      "Epoch: 20, Batch_num: 0, Time = 0.065608501, Training_loss = 1.55897\n",
      "Epoch: 20, Batch_num: 100, Time = 5.286200762, Training_loss = 1.43399\n",
      "Epoch: 20, Batch_num: 200, Time = 5.276240826, Training_loss = 1.42987\n",
      "Epoch: 20, Batch_num: 300, Time = 5.294924974, Training_loss = 1.44015\n",
      "Epoch: 21, Batch_num: 0, Time = 0.064073086, Training_loss = 1.55520\n",
      "Epoch: 21, Batch_num: 100, Time = 5.291359425, Training_loss = 1.43200\n",
      "Epoch: 21, Batch_num: 200, Time = 5.301984072, Training_loss = 1.42526\n",
      "Epoch: 21, Batch_num: 300, Time = 5.300156116, Training_loss = 1.43670\n",
      "Epoch: 22, Batch_num: 0, Time = 0.064446926, Training_loss = 1.55333\n",
      "Epoch: 22, Batch_num: 100, Time = 5.292092800, Training_loss = 1.43541\n",
      "Epoch: 22, Batch_num: 200, Time = 5.288857460, Training_loss = 1.42304\n",
      "Epoch: 22, Batch_num: 300, Time = 5.256180763, Training_loss = 1.43661\n",
      "Epoch: 23, Batch_num: 0, Time = 0.063638687, Training_loss = 1.55201\n",
      "Epoch: 23, Batch_num: 100, Time = 5.250156879, Training_loss = 1.43024\n",
      "Epoch: 23, Batch_num: 200, Time = 5.247814894, Training_loss = 1.42203\n",
      "Epoch: 23, Batch_num: 300, Time = 5.265294790, Training_loss = 1.43470\n",
      "Epoch: 24, Batch_num: 0, Time = 0.062370539, Training_loss = 1.55064\n",
      "Epoch: 24, Batch_num: 100, Time = 5.276013374, Training_loss = 1.42940\n",
      "Epoch: 24, Batch_num: 200, Time = 5.254917383, Training_loss = 1.41932\n",
      "Epoch: 24, Batch_num: 300, Time = 5.250176907, Training_loss = 1.43517\n",
      "Epoch: 25, Batch_num: 0, Time = 0.063689232, Training_loss = 1.55004\n",
      "Epoch: 25, Batch_num: 100, Time = 5.278378725, Training_loss = 1.42859\n",
      "Epoch: 25, Batch_num: 200, Time = 5.268851757, Training_loss = 1.42093\n",
      "Epoch: 25, Batch_num: 300, Time = 5.274803162, Training_loss = 1.43231\n",
      "Epoch: 26, Batch_num: 0, Time = 0.064696074, Training_loss = 1.54725\n",
      "Epoch: 26, Batch_num: 100, Time = 5.272745132, Training_loss = 1.42689\n",
      "Epoch: 26, Batch_num: 200, Time = 5.272087574, Training_loss = 1.41979\n",
      "Epoch: 26, Batch_num: 300, Time = 5.271549702, Training_loss = 1.43123\n",
      "Epoch: 27, Batch_num: 0, Time = 0.066172361, Training_loss = 1.54320\n",
      "Epoch: 27, Batch_num: 100, Time = 5.275742054, Training_loss = 1.42544\n",
      "Epoch: 27, Batch_num: 200, Time = 5.251484632, Training_loss = 1.41598\n",
      "Epoch: 27, Batch_num: 300, Time = 5.276094198, Training_loss = 1.43012\n",
      "Epoch: 28, Batch_num: 0, Time = 0.067123652, Training_loss = 1.54593\n",
      "Epoch: 28, Batch_num: 100, Time = 5.277183771, Training_loss = 1.42145\n",
      "Epoch: 28, Batch_num: 200, Time = 5.262518167, Training_loss = 1.41495\n",
      "Epoch: 28, Batch_num: 300, Time = 5.298822403, Training_loss = 1.42816\n",
      "Epoch: 29, Batch_num: 0, Time = 0.066232681, Training_loss = 1.54176\n",
      "Epoch: 29, Batch_num: 100, Time = 5.303906679, Training_loss = 1.41937\n",
      "Epoch: 29, Batch_num: 200, Time = 5.285710096, Training_loss = 1.41344\n",
      "Epoch: 29, Batch_num: 300, Time = 5.164081573, Training_loss = 1.42677\n",
      "Epoch: 30, Batch_num: 0, Time = 0.064049959, Training_loss = 1.54098\n",
      "Epoch: 30, Batch_num: 100, Time = 5.279963493, Training_loss = 1.41989\n",
      "Epoch: 30, Batch_num: 200, Time = 5.276438236, Training_loss = 1.41437\n",
      "Epoch: 30, Batch_num: 300, Time = 5.265929937, Training_loss = 1.42767\n",
      "Epoch: 31, Batch_num: 0, Time = 0.065731525, Training_loss = 1.54681\n",
      "Epoch: 31, Batch_num: 100, Time = 5.254372835, Training_loss = 1.41668\n",
      "Epoch: 31, Batch_num: 200, Time = 5.280196905, Training_loss = 1.41357\n",
      "Epoch: 31, Batch_num: 300, Time = 5.271507740, Training_loss = 1.42300\n",
      "Epoch: 32, Batch_num: 0, Time = 0.062725067, Training_loss = 1.54682\n",
      "Epoch: 32, Batch_num: 100, Time = 5.244246721, Training_loss = 1.41438\n",
      "Epoch: 32, Batch_num: 200, Time = 5.256506920, Training_loss = 1.41299\n",
      "Epoch: 32, Batch_num: 300, Time = 5.282014847, Training_loss = 1.42344\n",
      "Epoch: 33, Batch_num: 0, Time = 0.062375307, Training_loss = 1.54339\n",
      "Epoch: 33, Batch_num: 100, Time = 5.286219358, Training_loss = 1.41211\n",
      "Epoch: 33, Batch_num: 200, Time = 5.222015619, Training_loss = 1.41361\n",
      "Epoch: 33, Batch_num: 300, Time = 5.149706364, Training_loss = 1.42213\n",
      "Epoch: 34, Batch_num: 0, Time = 0.062912226, Training_loss = 1.54291\n",
      "Epoch: 34, Batch_num: 100, Time = 5.244277716, Training_loss = 1.41091\n",
      "Epoch: 34, Batch_num: 200, Time = 5.261392355, Training_loss = 1.41184\n",
      "Epoch: 34, Batch_num: 300, Time = 5.246478796, Training_loss = 1.42324\n",
      "Epoch: 35, Batch_num: 0, Time = 0.064909458, Training_loss = 1.54362\n",
      "Epoch: 35, Batch_num: 100, Time = 5.299711704, Training_loss = 1.41187\n",
      "Epoch: 35, Batch_num: 200, Time = 5.294847727, Training_loss = 1.41206\n",
      "Epoch: 35, Batch_num: 300, Time = 5.274853230, Training_loss = 1.42339\n",
      "Epoch: 36, Batch_num: 0, Time = 0.062512875, Training_loss = 1.54597\n",
      "Epoch: 36, Batch_num: 100, Time = 5.266092300, Training_loss = 1.41070\n",
      "Epoch: 36, Batch_num: 200, Time = 5.270301819, Training_loss = 1.41094\n",
      "Epoch: 36, Batch_num: 300, Time = 5.299032927, Training_loss = 1.41974\n",
      "Epoch: 37, Batch_num: 0, Time = 0.064805269, Training_loss = 1.53999\n",
      "Epoch: 37, Batch_num: 100, Time = 5.263586998, Training_loss = 1.40730\n",
      "Epoch: 37, Batch_num: 200, Time = 5.283634186, Training_loss = 1.40859\n",
      "Epoch: 37, Batch_num: 300, Time = 5.302362680, Training_loss = 1.42087\n",
      "Epoch: 38, Batch_num: 0, Time = 0.063167810, Training_loss = 1.53659\n",
      "Epoch: 38, Batch_num: 100, Time = 5.271627665, Training_loss = 1.40713\n",
      "Epoch: 38, Batch_num: 200, Time = 5.283890724, Training_loss = 1.40556\n",
      "Epoch: 38, Batch_num: 300, Time = 5.266593218, Training_loss = 1.41866\n",
      "Epoch: 39, Batch_num: 0, Time = 0.068143845, Training_loss = 1.53463\n",
      "Epoch: 39, Batch_num: 100, Time = 5.288749695, Training_loss = 1.40383\n",
      "Epoch: 39, Batch_num: 200, Time = 5.296973228, Training_loss = 1.40401\n",
      "Epoch: 39, Batch_num: 300, Time = 5.288056850, Training_loss = 1.41852\n",
      "Epoch: 40, Batch_num: 0, Time = 0.062128305, Training_loss = 1.53709\n",
      "Epoch: 40, Batch_num: 100, Time = 5.262900829, Training_loss = 1.40691\n",
      "Epoch: 40, Batch_num: 200, Time = 5.257602453, Training_loss = 1.40274\n",
      "Epoch: 40, Batch_num: 300, Time = 5.252600908, Training_loss = 1.41740\n",
      "Epoch: 41, Batch_num: 0, Time = 0.063605785, Training_loss = 1.53638\n",
      "Epoch: 41, Batch_num: 100, Time = 5.283068657, Training_loss = 1.40862\n",
      "Epoch: 41, Batch_num: 200, Time = 5.275621653, Training_loss = 1.40227\n",
      "Epoch: 41, Batch_num: 300, Time = 5.284651756, Training_loss = 1.41758\n",
      "Epoch: 42, Batch_num: 0, Time = 0.062519550, Training_loss = 1.53313\n",
      "Epoch: 42, Batch_num: 100, Time = 5.309503794, Training_loss = 1.40965\n",
      "Epoch: 42, Batch_num: 200, Time = 5.289499283, Training_loss = 1.40180\n",
      "Epoch: 42, Batch_num: 300, Time = 5.278794050, Training_loss = 1.41527\n",
      "Epoch: 43, Batch_num: 0, Time = 0.064105034, Training_loss = 1.53501\n",
      "Epoch: 43, Batch_num: 100, Time = 5.270370245, Training_loss = 1.40806\n",
      "Epoch: 43, Batch_num: 200, Time = 5.265301228, Training_loss = 1.40098\n",
      "Epoch: 43, Batch_num: 300, Time = 5.293601751, Training_loss = 1.41414\n",
      "Epoch: 44, Batch_num: 0, Time = 0.064588547, Training_loss = 1.53840\n",
      "Epoch: 44, Batch_num: 100, Time = 5.283010721, Training_loss = 1.40525\n",
      "Epoch: 44, Batch_num: 200, Time = 5.287383080, Training_loss = 1.39847\n",
      "Epoch: 44, Batch_num: 300, Time = 5.258035660, Training_loss = 1.41320\n",
      "Epoch: 45, Batch_num: 0, Time = 0.064859390, Training_loss = 1.53798\n",
      "Epoch: 45, Batch_num: 100, Time = 5.282611132, Training_loss = 1.40560\n",
      "Epoch: 45, Batch_num: 200, Time = 5.270881653, Training_loss = 1.40021\n",
      "Epoch: 45, Batch_num: 300, Time = 5.264379501, Training_loss = 1.41467\n",
      "Epoch: 46, Batch_num: 0, Time = 0.065001488, Training_loss = 1.53486\n",
      "Epoch: 46, Batch_num: 100, Time = 5.272251368, Training_loss = 1.40693\n",
      "Epoch: 46, Batch_num: 200, Time = 5.287592649, Training_loss = 1.39839\n",
      "Epoch: 46, Batch_num: 300, Time = 5.279085159, Training_loss = 1.41150\n",
      "Epoch: 47, Batch_num: 0, Time = 0.065893888, Training_loss = 1.53225\n",
      "Epoch: 47, Batch_num: 100, Time = 5.280922890, Training_loss = 1.40642\n",
      "Epoch: 47, Batch_num: 200, Time = 5.293931961, Training_loss = 1.39410\n",
      "Epoch: 47, Batch_num: 300, Time = 5.279056311, Training_loss = 1.41225\n",
      "Epoch: 48, Batch_num: 0, Time = 0.064353704, Training_loss = 1.52898\n",
      "Epoch: 48, Batch_num: 100, Time = 5.264434338, Training_loss = 1.40518\n",
      "Epoch: 48, Batch_num: 200, Time = 5.312371254, Training_loss = 1.39467\n",
      "Epoch: 48, Batch_num: 300, Time = 5.293295383, Training_loss = 1.41126\n",
      "Epoch: 49, Batch_num: 0, Time = 0.065076351, Training_loss = 1.52501\n",
      "Epoch: 49, Batch_num: 100, Time = 5.276140213, Training_loss = 1.40456\n",
      "Epoch: 49, Batch_num: 200, Time = 5.280132294, Training_loss = 1.39955\n",
      "Epoch: 49, Batch_num: 300, Time = 5.296588898, Training_loss = 1.41044\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "batch_data = text_loader.generate_batches()\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print (sess.graph)\n",
    "    print (tf.trainable_variables())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        \n",
    "        state = sess.run(model.initial_state)\n",
    "        num_batch = 0\n",
    "        start_time = time.time()\n",
    "        batch_data = text_loader.generate_batches()\n",
    "\n",
    "        for x, y in batch_data:\n",
    "            ops = [model.final_loss, model.final_state, model.train_op]\n",
    "            feed = {model.inputs: x, model.targets: y, model.initial_state: state}\n",
    "            loss, state, _ = sess.run(ops, feed)\n",
    "            if num_batch % 100 == 0:\n",
    "                end_time = time.time()  \n",
    "                print (\"Epoch: {}, Batch_num: {}, Time = {:.9f}, Training_loss = {:.5f}\".format(i, num_batch, end_time-start_time, loss))\n",
    "                start_time = end_time\n",
    "                \n",
    "            num_batch += 1\n",
    "\n",
    "    ckpt_path = os.path.join(MODEL_DIR, 'model.ckpt')\n",
    "    saver.save(sess, ckpt_path, global_step= i * text_loader.num_batches + num_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The generated text is at the end of the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLOI\n",
      "model_checkpoint_path: \"/home/jupyter/HW3_template/model_new.ckpt-17850\"\n",
      "all_model_checkpoint_paths: \"/home/jupyter/HW3_template/model_new.ckpt-17850\"\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/HW3_template/model_new.ckpt-17850\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "To the people, beg their heads,\n",
      "And the sea of the common prince and the s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "is_training = False\n",
    "model = Model(batch_size, seq_len, vocab_size, rnn_size, rnn_type, num_layers, use_dropout, lr, is_training, input_keep_prob, outptut_keep_prob, use_embedding, embedding_size, max_grad_clip)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(MODEL_DIR)\n",
    "    print (ckpt)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    pred_seq = model.sample(sess, text_loader.char, text_loader.vocab,  50, \"To the people, beg their\")\n",
    "    print (pred_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLOI\n",
      "model_checkpoint_path: \"/home/jupyter/HW3_template/model.ckpt-17850\"\n",
      "all_model_checkpoint_paths: \"/home/jupyter/HW3_template/model.ckpt-17850\"\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/HW3_template/model.ckpt-17850\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "To the princess\n",
      "That the most prophesy of the sea cannot be a man\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "is_training = False\n",
    "model = Model(batch_size, seq_len, vocab_size, rnn_size, rnn_type, num_layers, use_dropout, lr, is_training, input_keep_prob, outptut_keep_prob, use_embedding, embedding_size, max_grad_clip)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(MODEL_DIR)\n",
    "    print (ckpt)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    pred_seq = model.sample(sess, text_loader.char, text_loader.vocab,  50, \"To the princess\")\n",
    "    print (pred_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
